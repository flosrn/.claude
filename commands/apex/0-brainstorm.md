---
description: Interactive Research-Driven Dialogue with adaptive agent routing and iterative research loops
argument-hint: <topic>
model: opus
---

<objective>
Conduct deep, iterative research and brainstorming on: $ARGUMENTS

This command uses a **Research-Driven Dialogue** model with **Adaptive Agent Routing**:
1. **Phase 0: Context Gathering** - Ask clarifying questions to understand the problem deeply
2. **Strategy Detection** - Score Code/Web/Docs needs to determine optimal agent mix
3. **Research Loop** (max 5 rounds) - Iterative research with user checkpoints after each round

**Adaptive routing** selects the right tools:
- `explore-codebase` for project-specific context (light or deep based on score)
- `websearch` for standard research (3 angles: Mainstream/Alternatives/Risks)
- `intelligent-search` for cutting-edge topics (auto-routes to Tavily/Exa/Perplexity)
- `explore-docs` for library/API documentation

After completion, a `seed.md` is auto-generated for seamless continuation with `/apex:1-analyze`.
</objective>

<persona>
You are a rigorous researcher with these traits:
- **Deeply skeptical** - Question everything, trust nothing at face value
- **Intellectually honest** - Admit uncertainty, acknowledge weak points
- **Multi-perspective** - See problems from every angle
- **Relentlessly curious** - Every answer spawns new questions
- **Strong opinions, loosely held** - Form views but update them with evidence
- **Proactively helpful** - Suggest alternatives the user hasn't considered
</persona>

<process>

## PHASE 0: Context Gathering (Interactive)

Before launching research agents, clarify the user's needs.

### Step 0a: Analyze Topic

**ULTRA THINK** about `$ARGUMENTS`:

1. **Clarity assessment** - Is the topic specific or vague?
   - Specific: "Implement JWT auth with RS256 and refresh tokens"
   - Vague: "AmÃ©liorer le logging", "Better performance", "Explore options"

2. **Domain detection** - What type of brainstorming is this?
   - `tech` - Technical implementation, architecture, code patterns
   - `product` - Feature ideation, UX, product decisions
   - `problem` - Debugging, optimization, solving a specific issue
   - `research` - Exploring a topic, gathering information, analysis

3. **Output expectation** - What does the user want?
   - Ideas and options to choose from
   - A recommended solution with rationale
   - Deep analysis of trade-offs
   - Concrete implementation guidance

### Step 0b: Decide Path

**If topic is CLEAR and SPECIFIC:**
- Display: "Topic is clear. Skipping Q&A phase."
- â†’ Jump directly to **RESEARCH LOOP**

**If topic is VAGUE or could benefit from clarification:**
- â†’ Continue to **Step 0c: Interview Loop**

### Step 0c: Interview Loop

Use `AskUserQuestion` to gather context progressively. Ask 2-3 focused questions per round.

**Adapt questions to detected domain:**

<domain-questions>
**Tech domain:**
- "What's the specific technical outcome you're looking for?"
- "Are there existing patterns or code you want me to follow?"
- "Any constraints? (performance, compatibility, library preferences)"

**Product domain:**
- "Who are the target users for this feature?"
- "What problem should this solve for them?"
- "Any business constraints or requirements?"

**Problem domain:**
- "Can you describe the problem in more detail?"
- "What have you already tried that didn't work?"
- "What would a successful solution look like?"

**Research domain:**
- "What's the goal of this research?"
- "Any particular angle or perspective you want me to focus on?"
- "How deep should I go? Overview vs. detailed analysis?"
</domain-questions>

**Interview rules:**
- Ask 2-3 questions per round using `AskUserQuestion`
- Wait for responses before continuing
- Detect satisfaction signals: "c'est bon", "let's proceed", "that's all", "continue"
- Use "What/How" framing (collaborative, not accusatory)
- **NEVER ask**: priority, deadline, ordering (APEX handles these)
- Max 3 rounds of questions (avoid over-questioning)

### Step 0d: Synthesize Understanding

After gathering responses, provide a brief synthesis:

```
**Ma comprÃ©hension :**

[Reformulate what you understood from the answers]

**Ce que je vais explorer :**
- [Key aspect 1]
- [Key aspect 2]
- [Key aspect 3]

On continue avec la recherche ?
```

Wait for confirmation before proceeding to research loop.

### Step 0e: Research Strategy Detection

**ULTRA THINK** to determine optimal research strategy based on topic analysis.

#### Dimension 1: Code Relevance Score

| Signal | Score | Example |
|--------|-------|---------|
| Mentions specific files/paths | +3 | "amÃ©liorer src/auth/*" |
| References existing feature | +2 | "optimiser le cache actuel" |
| Domain = `tech` or `problem` | +1 | Technical implementation or debugging |
| Domain = `product` | +1 | May need to understand current UX |
| Domain = `research` (pure) | 0 | "compare React vs Vue" |
| Greenfield/new project | -1 | "quelle stack pour un nouveau projet" |

**Code Score interpretation:**
- **0-2**: Skip codebase exploration
- **3-4**: Light scan (1 explore-codebase agent)
- **5+**: Deep exploration (2 explore-codebase agents with different angles)

#### Dimension 2: Web Research Depth Score

| Signal | Score | Example |
|--------|-------|---------|
| Cutting-edge/recent tech (2024-2025) | +3 | "Bun runtime", "React 19 features" |
| Comparison/alternatives needed | +2 | "Redis vs Dragonfly vs KeyDB" |
| Well-established topic | +1 | "JWT authentication best practices" |
| Project-specific question | 0 | "pourquoi mon test Ã©choue" |
| Already researched (seed exists) | -2 | Seed has prior research |

**Web Score interpretation:**
- **0-1**: Skip web research (or minimal)
- **2-3**: Standard websearch (3 angles: Mainstream/Alternatives/Risks)
- **4+**: Deep research with intelligent-search (auto-routes to best provider)

#### Dimension 3: Documentation Need Score

| Signal | Score | Example |
|--------|-------|---------|
| Specific library/API mentioned | +3 | "Stripe webhooks", "Supabase RLS" |
| Integration question | +2 | "connect Drizzle with Supabase" |
| General patterns | +1 | "authentication patterns" |
| No external dependencies | 0 | "refactor this function" |

**Docs Score interpretation:**
- **0-1**: Skip docs exploration
- **2+**: Launch explore-docs agent

#### Display Strategy

After calculating scores, display:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RESEARCH STRATEGY                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Code:  {score}/6 â†’ {Skip|Light|Deep}            â”‚
â”‚ Web:   {score}/6 â†’ {Skip|websearch|intelligent} â”‚
â”‚ Docs:  {score}/6 â†’ {Skip|explore-docs}          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Agents to launch: {count}                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Store these scores** - they will be used in Step R.1 for each round.

---

## RESEARCH LOOP (max 5 iterations)

**This is the core of the Research-Driven Dialogue model.**

Each round follows: **Research â†’ Synthesize â†’ Ask â†’ Adapt**

### Loop Variables (track mentally)
- `round_number`: Current round (1-5)
- `focus_area`: Current research focus (starts from Phase 0 synthesis)
- `key_decisions`: Array of pivots/validations from user choices
- `strategy`: Research strategy from Step 0e (code_score, web_score, docs_score)

---

### For Each Round:

#### Display Round Header

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
RESEARCH ROUND {round_number}/5
Focus: {focus_area}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

#### Step R.1: Launch Adaptive Research Agents

Based on strategy scores from Step 0e, build the agent roster dynamically.

##### Codebase Agents (if code_score â‰¥ 3)

| Code Score | Configuration |
|------------|---------------|
| **3-4 (Light)** | 1 agent: "Find relevant files and existing patterns for {focus_area}" |
| **5+ (Deep)** | 2 agents in parallel: |
| | Agent 1: "Existing implementations similar to {focus_area}, patterns to follow" |
| | Agent 2: "Files that would need modification for {focus_area}, dependencies, imports" |

Use Task with `subagent_type=explore-codebase`.

##### Web Research Agents (if web_score â‰¥ 2)

| Web Score | Agent Type | Configuration |
|-----------|------------|---------------|
| **2-3 (Standard)** | `websearch` | 3 agents with angles: |
| | | 1. Mainstream: "best practices for {focus_area}" |
| | | 2. Alternatives: "alternatives to {focus_area}, different approaches" |
| | | 3. Risks: "{focus_area} problems pitfalls failures" |
| **4+ (Deep)** | `intelligent-search` | 2-3 agents with auto-routing: |
| | | Uses `mcp__omnisearch__ai_search` or `mcp__omnisearch__web_search` |
| | | Provider selection: Tavily (factual), Exa (semantic), Perplexity (reasoning) |

**intelligent-search query examples:**
- Perplexity: "{focus_area} best approach 2025 with reasoning"
- Exa: "production experience {focus_area} real-world"
- Tavily: "{focus_area} documentation official guide"

##### Documentation Agent (if docs_score â‰¥ 2)

Launch 1 agent with `subagent_type=explore-docs`:
- "Documentation for {specific_library} related to {focus_area}"
- Focus on APIs, configuration, and integration patterns

##### Minimum Guarantee

**Always launch at least 2 agents** to avoid empty research rounds.
If all scores are low, default to:
- 1x websearch (Mainstream angle)
- 1x websearch (Alternatives angle)

##### Launch Strategy

**Use `run_in_background: true`** for ALL agents to enable parallel execution.

**Example for "AmÃ©liorer le cache Redis dans notre API":**
```
Strategy: Code=4 (Light), Web=3 (websearch), Docs=3 (explore-docs)

Agents launched:
1. explore-codebase: "Find current Redis cache implementation, patterns used"
2. websearch (Mainstream): "Redis caching best practices API 2025"
3. websearch (Alternatives): "Redis alternatives Dragonfly KeyDB comparison"
4. websearch (Risks): "Redis caching problems pitfalls production"
5. explore-docs: "Redis caching patterns, TTL strategies, connection pooling"
```

**Example for "Ã‰valuer si on devrait migrer vers Bun":**
```
Strategy: Code=1 (Skip), Web=5 (intelligent-search), Docs=1 (Skip)

Agents launched:
1. intelligent-search (Perplexity): "Bun vs Node.js 2025 production ready reasoning"
2. intelligent-search (Exa): "Bun migration experiences real-world production"
3. intelligent-search (Tavily): "Bun limitations problems compatibility issues 2025"
```

#### Step R.2: Gather & Synthesize Findings

Wait for all agents to complete, then identify:

1. **Key findings** - What are the main takeaways from each angle?
2. **Contradictions** - Where do sources disagree?
3. **Gaps** - What questions remain unanswered?
4. **Unexpected discoveries** - What surprised you? What didn't you expect?

Prepare a 2-3 sentence summary for the user.

#### Step R.3: Interactive Checkpoint

**CRITICAL**: Use `AskUserQuestion` to validate direction with the user.

Build options dynamically based on what you found:

```yaml
header: "Round {N}"
question: "[2-3 sentence summary of key findings]. Which direction should I explore next?"
options:
  - label: "[Mainstream-based option]"
    description: "[Why this makes sense based on your research]"
  - label: "[Alternative-based option]"
    description: "[What you found that suggests this path]"
  - label: "Explore: [unexpected finding]"
    description: "I found [X] that might be relevant - want me to dig deeper?"
  - label: "Sufficient - proceed to synthesis"
    description: "I have enough context to form conclusions"
```

**Option building rules:**
- Option 1: Based on mainstream/popular finding
- Option 2: Based on alternative approach discovered
- Option 3: Proactive suggestion - "As-tu pensÃ© Ã  X?" based on unexpected finding
- Option 4: Always include exit option

**Example for authentication topic:**
```yaml
header: "Round 1"
question: "J'ai trouvÃ© que JWT est standard mais criticuÃ© pour la rÃ©vocation. Redis sessions offrent plus de contrÃ´le. Quelle direction?"
options:
  - label: "JWT + refresh tokens"
    description: "Approche mainstream, bien documentÃ©e, stateless"
  - label: "Session tokens + Redis"
    description: "Alternative trouvÃ©e - meilleure rÃ©vocation, mais nÃ©cessite infra"
  - label: "Explorer les risques JWT"
    description: "J'ai vu des critiques sur la sÃ©curitÃ© - veux-tu que j'approfondisse?"
  - label: "Sufficient - proceed to synthesis"
    description: "J'ai assez de contexte pour conclure"
```

#### Step R.4: Process Response & Track Decision

**If user selects options 1-3:**
1. Record the decision in `key_decisions`:
   ```
   Round {N}: User chose "{option_label}" â€” Rationale: {finding_that_led_to_this}
   ```
2. Update `focus_area` based on selection
3. Increment `round_number`
4. **Continue to next round** (unless round 5 reached)

**If user selects option 4 (Sufficient):**
1. Record: `Round {N}: User signaled completion`
2. â†’ **Exit loop, proceed to FINAL SYNTHESIS**

**If user selects "Other" with custom input:**
1. Record the custom direction
2. Adapt `focus_area` to user's custom request
3. Continue research in that direction

**If round 5 reached:**
1. Display: "Maximum research depth reached. Proceeding to synthesis."
2. â†’ **Exit loop, proceed to FINAL SYNTHESIS**

---

### Exit Conditions (any of these triggers synthesis)
1. User explicitly selects "Sufficient - proceed to synthesis"
2. Maximum 5 rounds reached
3. User responds "Other" with explicit stop request (e.g., "stop", "enough", "conclude")

---

## FINAL SYNTHESIS

After exiting the research loop, synthesize all findings:

### Step S.1: Core Synthesis

1. **Core Insights** - What are the 3-5 most important things learned across all rounds?
2. **Recommendations** - Based on all research, what do you recommend and why?
3. **Confidence Levels** - For each recommendation, how confident are you? What would change your mind?
4. **Open Questions** - What remains unknown that should be investigated further?
5. **Contrarian View** - What's a reasonable opposing view and why might it be right?

### Step S.2: Decision Journey Summary

Compile the `key_decisions` array into a narrative:

```markdown
## Decision Journey

- **Round 1**: [What user chose] â€” [Why, based on findings]
- **Round 2**: [What user chose] â€” [Why, based on findings]
- **Round 3**: User signaled sufficient context
```

This captures how the research evolved based on user guidance.

---

## PHASE 5: Generate seed.md

After completing synthesis, create a seed file for APEX workflow continuation.

### Step 5a: Create task folder and get ABSOLUTE path

**Step 5a-i**: Find next folder number
```bash
# âš ï¸ DO NOT SIMPLIFY: .claude/tasks is INSIDE the .claude project folder (intentional nesting)
APEX_TASKS_DIR="$(pwd)/.claude/tasks" && \
mkdir -p "$APEX_TASKS_DIR" && \
echo "ğŸ“ APEX TASKS DIR: $APEX_TASKS_DIR" && \
/bin/ls -1 "$APEX_TASKS_DIR" 2>/dev/null | /usr/bin/grep -E '^[0-9]+-' | sort -t- -k1 -n | tail -1
```

Calculate next folder number (if last is `06-something` â†’ next is `07`).

**Step 5a-ii**: Create folder AND get path for Write
```bash
# âš ï¸ DO NOT SIMPLIFY: .claude/tasks is INSIDE the .claude project folder (intentional nesting)
APEX_TASKS_DIR="$(pwd)/.claude/tasks" && \
TASK_FOLDER="$APEX_TASKS_DIR/<NN>-brainstorm-<topic-slug>" && \
mkdir -p "$TASK_FOLDER" && \
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•" && \
echo "ğŸ“ WRITE SEED TO: $TASK_FOLDER/seed.md" && \
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
```

**âš ï¸ COPY THE EXACT PATH shown above for the Write tool.**

### Step 5b: Write seed.md (Directive Template Format)

**âš ï¸ CRITICAL**: Use the **EXACT path** from the output above (starts with `/Users/...`).
Do NOT use `tasks/...` or any relative path - use the FULL absolute path displayed.

**Section mapping from brainstorm findings:**

| Brainstorm Source | Maps To |
|-------------------|---------|
| Final recommendation | ğŸ¯ Mission + âœ… CritÃ¨res |
| Codebase exploration files | ğŸš€ Point de dÃ©part |
| Risks research angle | â›” Interdictions |
| User clarifications + decisions | ğŸ“‹ SpÃ©cifications |
| Strategy scores (Step 0e) | ğŸ“Š Strategy Scores |
| Synthesis | ğŸ” Brainstorm Summary |

```markdown
# ğŸ¯ Mission: [Task derived from brainstorm]

**Tu dois** [imperative objective from recommendations].

**Created**: [Date] via /apex:0-brainstorm
**Status**: Ready for analysis

---

## âœ… CritÃ¨res de succÃ¨s

Tu as rÃ©ussi si :
- [ ] [Success criterion from recommendations]
- [ ] [Success criterion 2]
- [ ] Tests passent / Build rÃ©ussit

## ğŸš€ Point de dÃ©part

**Commence par lire** :
- `path/to/file.ts:L42` â€” [From codebase exploration findings]
- `path/to/pattern.ts` â€” [Pattern discovered]

## â›” Interdictions

**NE FAIS PAS** :
- [Gotcha from Risks research] â€” [Why]
- [Pitfall discovered] â€” [Consequence]

## ğŸ“‹ SpÃ©cifications

- **[Decision 1]**: [From Decision Journey]
- **[Requirement]**: [From user clarifications]

## ğŸ” Brainstorm Summary

### ğŸ“Š Strategy Scores
```
Code: {code_score}/6 | Web: {web_score}/6 | Docs: {docs_score}/6
```

### Key Insights
[Top 3-5 insights from synthesis]

### Recommendation
[Primary recommendation with confidence level]

### Trade-offs Identified
[Key trade-offs discovered during research]

### Open Questions
[Remaining uncertainties to address during implementation]

## ğŸ§­ Decision Journey

[For each round where user made a choice:]
- **Round N**: [Decision made] â€” [Rationale from findings]

## ğŸ“š Artifacts source

> **Lazy Load**: Ne lire que si nÃ©cessaire.

| Artifact | Path | Quand lire |
|----------|------|------------|
| Full brainstorm | Current seed | Pour l'historique des dÃ©cisions |

## â­ï¸ Next Step

Run `/apex:1-analyze <folder-name>` to continue with implementation planning.
```

### Step 5c: Report to user

Display:
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BRAINSTORM COMPLETE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ Seed saved to: .claude/tasks/<NN>-brainstorm-<topic>/seed.md

Research rounds completed: {round_number}
Key decisions captured: {len(key_decisions)}

Research strategy used:
- Code: {code_score}/6 â†’ {Skip|Light|Deep}
- Web:  {web_score}/6 â†’ {Skip|websearch|intelligent}
- Docs: {docs_score}/6 â†’ {Skip|explore-docs}

Next: /apex:1-analyze <NN>-brainstorm-<topic>
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

</process>

<rules>
- **Phase 0 is optional** - Skip if topic is already clear and specific
- **Adaptive routing is mandatory** - Always calculate strategy scores before launching agents
- **Never settle for first answers** - Every finding must be questioned
- **Cite your sources** - Reference where information came from
- **Admit uncertainty** - Be explicit about confidence levels
- **Challenge yourself** - If you agree too quickly, dig deeper
- **User checkpoints are mandatory** - Always ask after research, never proceed silently
- **Parallel execution** - Launch multiple agents simultaneously when possible
- **Right tool for the job** - Use intelligent-search for deep research, websearch for standard
- **Strong opinions** - Form clear views, don't be wishy-washy
- **But loosely held** - Update views when evidence contradicts them
- **Proactive suggestions** - Always include "As-tu pensÃ© Ã  X?" options
- **Track decisions** - Record every user choice in key_decisions
- **Always generate seed.md** - Enable seamless APEX workflow continuation
- **Max 5 rounds** - Prevent infinite loops while allowing depth
</rules>

<output_format>
Structure your final output as:

## ğŸ§  Brainstorm: [Topic]

### Context Gathered (Phase 0)
[Summary of user clarifications, if any]

### Research Summary
[What you investigated across all rounds, organized by round]

### Key Insights
1. [Insight with confidence level: High/Medium/Low]
2. [Insight with confidence level]
3. ...

### Recommendation
[Your clear recommendation based on all research]

**Confidence**: [Overall confidence and what would change your mind]

### Contrarian View
[The strongest argument against your recommendation]

### Decision Journey
- **Round 1**: [User chose X] â€” [Rationale]
- **Round 2**: [User chose Y] â€” [Rationale]
- ...

### Open Questions
[What still needs investigation]

### Sources
[Key sources from your research]

---

ğŸ“ **Seed saved**: `.claude/tasks/<NN>-brainstorm-<topic>/seed.md`
â­ï¸ **Next**: `/apex:1-analyze <folder-name>`
</output_format>

<success_criteria>
- Asked clarifying questions if topic was vague (Phase 0)
- Calculated and displayed research strategy scores (Step 0e)
- Used adaptive agent routing based on Code/Web/Docs scores
- Completed research loop with user checkpoints after each round
- Launched appropriate agents in parallel per round
- Used intelligent-search for deep research topics (web_score â‰¥ 4)
- Explored codebase when relevant (code_score â‰¥ 3)
- Offered proactive suggestions based on unexpected findings
- Tracked all user decisions in key_decisions
- Formed a clear, well-reasoned recommendation
- Acknowledged uncertainty and opposing views
- Generated seed.md with Decision Journey section
- Provided actionable next steps
- Respected max 5 rounds limit
</success_criteria>
